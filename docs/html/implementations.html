<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Implementations &mdash; Fastinference  documentation</title>
      <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
      <link rel="stylesheet" href="_static/custom.css" type="text/css" />
  <!--[if lt IE 9]>
    <script src="_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
        <script src="_static/jquery.js"></script>
        <script src="_static/underscore.js"></script>
        <script src="_static/doctools.js"></script>
    <script src="_static/js/theme.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Extending fastinference" href="extending.html" />
    <link rel="prev" title="Neural Networks" href="neuralnet.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="index.html" class="icon icon-home"> Fastinference
            <img src="_static/logo-docs.png" class="logo" alt="Logo"/>
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <ul class="current">
<li class="toctree-l1"><a class="reference internal" href="index.html">Fastinference</a></li>
<li class="toctree-l1"><a class="reference internal" href="models.html">Models</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Implementations</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#linear-implementations">Linear implementations</a></li>
<li class="toctree-l2"><a class="reference internal" href="#discriminant-analysis-implementations">Discriminant analysis implementations</a></li>
<li class="toctree-l2"><a class="reference internal" href="#tree-implementations">Tree implementations</a></li>
<li class="toctree-l2"><a class="reference internal" href="#ensemble-implementations">Ensemble implementations</a></li>
<li class="toctree-l2"><a class="reference internal" href="#neural-network-implementations">Neural network implementations</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="extending.html">Extending fastinference</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">Fastinference</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content style-external-links">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="index.html" class="icon icon-home"></a> &raquo;</li>
      <li>Implementations</li>
      <li class="wy-breadcrumbs-aside">
              <a href="https://github.com/sbuschjaeger/fastinference/blob/master/docs/implementations.rst" class="fa fa-github"> Edit on GitHub</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="implementations">
<h1>Implementations<a class="headerlink" href="#implementations" title="Permalink to this headline"></a></h1>
<section id="linear-implementations">
<h2>Linear implementations<a class="headerlink" href="#linear-implementations" title="Permalink to this headline"></a></h2>
</section>
<section id="discriminant-analysis-implementations">
<h2>Discriminant analysis implementations<a class="headerlink" href="#discriminant-analysis-implementations" title="Permalink to this headline"></a></h2>
<dl class="py function">
<dt class="sig sig-object py" id="fastinference.implementations.discriminant.cpp.implement.to_implementation">
<span class="sig-prename descclassname"><span class="pre">fastinference.implementations.discriminant.cpp.implement.</span></span><span class="sig-name descname"><span class="pre">to_implementation</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">model</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">out_path</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">out_name</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">weight</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">namespace</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'FAST_INFERENCE'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">feature_type</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'double'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">label_type</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'double'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">internal_type</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'double'</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#fastinference.implementations.discriminant.cpp.implement.to_implementation" title="Permalink to this definition"></a></dt>
<dd><p>Generates a (unrolled) C++ implementation of the given DiscriminantAnalysis model. Unrolled means that parameters of the model are not stored in arrays but are directly translated into the source code. You can use this implementation by simply passing <code class="code docutils literal notranslate"><span class="pre">&quot;cpp&quot;</span></code> to the implement, e.g.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">loaded_model</span> <span class="o">=</span> <span class="n">fastinference</span><span class="o">.</span><span class="n">Loader</span><span class="o">.</span><span class="n">model_from_file</span><span class="p">(</span><span class="s2">&quot;/my/nice/model.json&quot;</span><span class="p">)</span>
<span class="n">loaded_model</span><span class="o">.</span><span class="n">implement</span><span class="p">(</span><span class="s2">&quot;/some/nice/place/&quot;</span><span class="p">,</span> <span class="s2">&quot;mymodel&quot;</span><span class="p">,</span> <span class="s2">&quot;cpp&quot;</span><span class="p">)</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>model</strong> (<a class="reference internal" href="discriminant.html#fastinference.models.DiscriminantAnalysis.DiscriminantAnalysis" title="fastinference.models.DiscriminantAnalysis.DiscriminantAnalysis"><em>DiscriminantAnalysis</em></a>) – The DiscriminantAnalysis model to be implemented</p></li>
<li><p><strong>out_path</strong> (<em>str</em>) – The folder in which the <code class="code docutils literal notranslate"><span class="pre">*.cpp</span></code> and <code class="code docutils literal notranslate"><span class="pre">*.h</span></code> files are stored.</p></li>
<li><p><strong>out_name</strong> (<em>str</em>) – The filenames.</p></li>
<li><p><strong>weight</strong> (<em>float</em><em>, </em><em>optional</em>) – The weight of this model inside an ensemble. The weight is ignored if it is 1.0, otherwise the prediction is scaled by the respective weight.. Defaults to 1.0.</p></li>
<li><p><strong>namespace</strong> (<em>str</em><em>, </em><em>optional</em>) – The namespace under which this model will be generated. Defaults to “FAST_INFERENCE”.</p></li>
<li><p><strong>feature_type</strong> (<em>str</em><em>, </em><em>optional</em>) – The data types of the input features. Defaults to “double”.</p></li>
<li><p><strong>label_type</strong> (<em>str</em><em>, </em><em>optional</em>) – The data types of the label. Defaults to “double”.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

</section>
<section id="tree-implementations">
<h2>Tree implementations<a class="headerlink" href="#tree-implementations" title="Permalink to this headline"></a></h2>
<dl class="py function">
<dt class="sig sig-object py" id="fastinference.implementations.tree.cpp.ifelse.implement.to_implementation">
<span class="sig-prename descclassname"><span class="pre">fastinference.implementations.tree.cpp.ifelse.implement.</span></span><span class="sig-name descname"><span class="pre">to_implementation</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">model</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">out_path</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">out_name</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">weight</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">namespace</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'FAST_INFERENCE'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">feature_type</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'double'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">label_type</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'double'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">kernel_budget</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">kernel_type</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">output_debug</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">target_compiler</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'g++'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">target_objdump</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'objdump'</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#fastinference.implementations.tree.cpp.ifelse.implement.to_implementation" title="Permalink to this definition"></a></dt>
<dd><p>Generates a (unrolled) C++ implementation of the given Tree model. Unrolled means that the tree is represented in an if-then-else structure without any arrays. You can use this implementation by simply passing <code class="code docutils literal notranslate"><span class="pre">&quot;cpp.ifelse&quot;</span></code> to the implement, e.g.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">loaded_model</span> <span class="o">=</span> <span class="n">fastinference</span><span class="o">.</span><span class="n">Loader</span><span class="o">.</span><span class="n">model_from_file</span><span class="p">(</span><span class="s2">&quot;/my/nice/model.json&quot;</span><span class="p">)</span>
<span class="n">loaded_model</span><span class="o">.</span><span class="n">implement</span><span class="p">(</span><span class="s2">&quot;/some/nice/place/&quot;</span><span class="p">,</span> <span class="s2">&quot;mymodel&quot;</span><span class="p">,</span> <span class="s2">&quot;cpp.ifelse&quot;</span><span class="p">)</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>model</strong> (<a class="reference internal" href="tree.html#fastinference.models.Tree.Tree" title="fastinference.models.Tree.Tree"><em>Tree</em></a>) – The Tree model to be implemented</p></li>
<li><p><strong>out_path</strong> (<em>str</em>) – The folder in which the <code class="code docutils literal notranslate"><span class="pre">*.cpp</span></code> and <code class="code docutils literal notranslate"><span class="pre">*.h</span></code> files are stored.</p></li>
<li><p><strong>out_name</strong> (<em>str</em>) – The filenames.</p></li>
<li><p><strong>weight</strong> (<em>float</em><em>, </em><em>optional</em>) – The weight of this model inside an ensemble. The weight is ignored if it is 1.0, otherwise the prediction is scaled by the respective weight. Defaults to 1.0.</p></li>
<li><p><strong>namespace</strong> (<em>str</em><em>, </em><em>optional</em>) – The namespace under which this model will be generated. Defaults to “FAST_INFERENCE”.</p></li>
<li><p><strong>feature_type</strong> (<em>str</em><em>, </em><em>optional</em>) – The data types of the input features. Defaults to “double”.</p></li>
<li><p><strong>label_type</strong> (<em>str</em><em>, </em><em>optional</em>) – The data types of the label. Defaults to “double”.</p></li>
<li><p><strong>quantize_splits</strong> (<em>str</em><em>, </em><em>optional</em>) – Can be [“rounding”, “fixed”] or None.</p></li>
<li><p><strong>kernel_budget</strong> (<em>int</em><em>, </em><em>optional</em>) – The budget in bytes which is allowed in a single kernel. Kernel optimizations are ignored if the budget None. Defaults to None.</p></li>
<li><p><strong>kernel_type</strong> (<em>str</em><em>, </em><em>optional</em>) – The type of kernel optimization. Can be {path, node, None}. Kernel optimizations are ignored if the kernel type is None. Defaults to None.</p></li>
<li><p><strong>output_debug</strong> (<em>bool</em><em>, </em><em>optional</em>) – If True outputs the given tree in the given folder in a json file called <cite>{model_name}_debug.json</cite>. Useful when debugging optimizations or loading the tree with another tool. Defaults to False.</p></li>
<li><p><strong>target_compiler</strong> (<em>str</em><em>, </em><em>optional</em>) – The compiler used for compiling the dummy code to determine node sizes. If you want to use a cross-compiler (e.g. <cite>arm-linux-gnueabihf-gcc</cite>) you can set the path here accordingly. Defaults to “g++”.</p></li>
<li><p><strong>target_objdump</strong> (<em>str</em><em>, </em><em>optional</em>) – The de-compiler used for de-compiling the dummy code to determine node sizes. If you want to use a cross-compiler (e.g. <cite>arm-linux-gnueabihf-gcc</cite>) you can set the path here accordingly. Defaults to “objdump”.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="fastinference.implementations.tree.cpp.native.implement.to_implementation">
<span class="sig-prename descclassname"><span class="pre">fastinference.implementations.tree.cpp.native.implement.</span></span><span class="sig-name descname"><span class="pre">to_implementation</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">model</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">out_path</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">out_name</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">weight</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">namespace</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'FAST_INFERENCE'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">feature_type</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'double'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">label_type</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'double'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">int_type</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'unsigned</span> <span class="pre">int'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">output_debug</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">infer_types</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">reorder_nodes</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">set_size</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">8</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">force_cacheline</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#fastinference.implementations.tree.cpp.native.implement.to_implementation" title="Permalink to this definition"></a></dt>
<dd><p>Generates a native C++ implementation of the given Tree model. Native means that the tree is represented in an array structure which is iterated via a while-loop. You can use this implementation by simply passing <code class="code docutils literal notranslate"><span class="pre">&quot;cpp.native&quot;</span></code> to the implement, e.g.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">loaded_model</span> <span class="o">=</span> <span class="n">fastinference</span><span class="o">.</span><span class="n">Loader</span><span class="o">.</span><span class="n">model_from_file</span><span class="p">(</span><span class="s2">&quot;/my/nice/model.json&quot;</span><span class="p">)</span>
<span class="n">loaded_model</span><span class="o">.</span><span class="n">implement</span><span class="p">(</span><span class="s2">&quot;/some/nice/place/&quot;</span><span class="p">,</span> <span class="s2">&quot;mymodel&quot;</span><span class="p">,</span> <span class="s2">&quot;cpp.native&quot;</span><span class="p">)</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>model</strong> (<a class="reference internal" href="tree.html#fastinference.models.Tree.Tree" title="fastinference.models.Tree.Tree"><em>Tree</em></a>) – The Tree model to be implemented</p></li>
<li><p><strong>out_path</strong> (<em>str</em>) – The folder in which the <code class="code docutils literal notranslate"><span class="pre">*.cpp</span></code> and <code class="code docutils literal notranslate"><span class="pre">*.h</span></code> files are stored.</p></li>
<li><p><strong>out_name</strong> (<em>str</em>) – The filenames.</p></li>
<li><p><strong>weight</strong> (<em>float</em><em>, </em><em>optional</em>) – The weight of this model inside an ensemble. The weight is ignored if it is 1.0, otherwise the prediction is scaled by the respective weight. Defaults to 1.0.</p></li>
<li><p><strong>namespace</strong> (<em>str</em><em>, </em><em>optional</em>) – The namespace under which this model will be generated. Defaults to “FAST_INFERENCE”.</p></li>
<li><p><strong>feature_type</strong> (<em>str</em><em>, </em><em>optional</em>) – The data types of the input features. Defaults to “double”.</p></li>
<li><p><strong>label_type</strong> (<em>str</em><em>, </em><em>optional</em>) – The data types of the label. Defaults to “double”.</p></li>
<li><p><strong>output_debug</strong> (<em>bool</em><em>, </em><em>optional</em>) – If True outputs the given tree in the given folder in a json file called <cite>{model_name}_debug.json</cite>. Useful when debugging optimizations or loading the tree with another tool. Defaults to False.</p></li>
<li><p><strong>infer_types</strong> (<em>bool</em><em>, </em><em>optional</em>) – If True then the smallest data type for index variables is inferred by the overall tree size. Otherwise “unsigned int” is used. Defaults to False.</p></li>
<li><p><strong>reorder_nodes</strong> (<em>bool</em><em>, </em><em>optional</em>) – If True then the nodes in the tree are reorder so that cache set size is respected. You can set the size of the cache set via set_size parameter. Defaults to False.</p></li>
<li><p><strong>set_size</strong> (<em>int</em><em>, </em><em>optional</em>) – The size of the cache set for if reorder_nodes is set to True. Defaults to 8.</p></li>
<li><p><strong>force_cacheline</strong> (<em>bool</em><em>, </em><em>optional</em>) – If True then “padding” nodes are introduced to fill the entire cache line. Defaults to False.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

</section>
<section id="ensemble-implementations">
<h2>Ensemble implementations<a class="headerlink" href="#ensemble-implementations" title="Permalink to this headline"></a></h2>
<dl class="py function">
<dt class="sig sig-object py" id="fastinference.implementations.ensemble.cpp.implement.to_implementation">
<span class="sig-prename descclassname"><span class="pre">fastinference.implementations.ensemble.cpp.implement.</span></span><span class="sig-name descname"><span class="pre">to_implementation</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">model</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">out_path</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">out_name</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">weight</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">namespace</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'FAST_INFERENCE'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">feature_type</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'double'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">label_type</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'double'</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#fastinference.implementations.ensemble.cpp.implement.to_implementation" title="Permalink to this definition"></a></dt>
<dd><p>Generates a C++ implementation of the given Ensemble model. This implementation simply calls the respective implementations of the base learners. You can use this implementation by simply passing <code class="code docutils literal notranslate"><span class="pre">&quot;cpp&quot;</span></code> to implement. To choose the implementation of the base-learners pass an additional option to implement:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">loaded_model</span> <span class="o">=</span> <span class="n">fastinference</span><span class="o">.</span><span class="n">Loader</span><span class="o">.</span><span class="n">model_from_file</span><span class="p">(</span><span class="s2">&quot;/my/nice/model.json&quot;</span><span class="p">)</span>
<span class="n">loaded_model</span><span class="o">.</span><span class="n">implement</span><span class="p">(</span><span class="s2">&quot;/some/nice/place/&quot;</span><span class="p">,</span> <span class="s2">&quot;mymodel&quot;</span><span class="p">,</span> <span class="s2">&quot;cpp&quot;</span><span class="p">,</span> <span class="s2">&quot;implementation.of.base.learners&quot;</span><span class="p">)</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>model</strong> (<a class="reference internal" href="ensemble.html#fastinference.models.Ensemble.Ensemble" title="fastinference.models.Ensemble.Ensemble"><em>Ensemble</em></a>) – The Ensemble model to be implemented</p></li>
<li><p><strong>out_path</strong> (<em>str</em>) – The folder in which the <code class="code docutils literal notranslate"><span class="pre">*.cpp</span></code> and <code class="code docutils literal notranslate"><span class="pre">*.h</span></code> files are stored.</p></li>
<li><p><strong>out_name</strong> (<em>str</em>) – The filenames.</p></li>
<li><p><strong>weight</strong> (<em>float</em><em>, </em><em>optional</em>) – The weight of this model inside an ensemble. The weight is ignored if it is 1.0, otherwise the prediction is scaled by the respective weight.. Defaults to 1.0.</p></li>
<li><p><strong>namespace</strong> (<em>str</em><em>, </em><em>optional</em>) – The namespace under which this model will be generated. Defaults to “FAST_INFERENCE”.</p></li>
<li><p><strong>feature_type</strong> (<em>str</em><em>, </em><em>optional</em>) – The data types of the input features. Defaults to “double”.</p></li>
<li><p><strong>label_type</strong> (<em>str</em><em>, </em><em>optional</em>) – The data types of the label. Defaults to “double”.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

</section>
<section id="neural-network-implementations">
<h2>Neural network implementations<a class="headerlink" href="#neural-network-implementations" title="Permalink to this headline"></a></h2>
<dl class="py function">
<dt class="sig sig-object py" id="fastinference.implementations.neuralnet.cpp.NHWC.implement.to_implementation">
<span class="sig-prename descclassname"><span class="pre">fastinference.implementations.neuralnet.cpp.NHWC.implement.</span></span><span class="sig-name descname"><span class="pre">to_implementation</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">model</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">out_path</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">out_name</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">weight</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">align</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">namespace</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'FAST_INFERENCE'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">feature_type</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'double'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">label_type</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'double'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">internal_type</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'double'</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#fastinference.implementations.neuralnet.cpp.NHWC.implement.to_implementation" title="Permalink to this definition"></a></dt>
<dd><p>Generates a C++ implementation of the given NeuralNet model. This implementation provides a NHWC layout for the convolution layers basically resulting in a structure like this:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">for</span> <span class="n">n</span> <span class="o">=</span> <span class="mf">1.</span><span class="o">.</span><span class="n">N</span><span class="p">:</span>
    <span class="k">for</span> <span class="n">h</span> <span class="o">=</span> <span class="mf">1.</span><span class="o">.</span><span class="n">H</span><span class="p">:</span>
        <span class="k">for</span> <span class="n">w</span> <span class="o">=</span> <span class="mf">1.</span><span class="o">.</span><span class="n">W</span><span class="p">:</span>
            <span class="k">for</span> <span class="n">c</span> <span class="o">=</span> <span class="mf">1.</span><span class="o">.</span><span class="n">C</span><span class="p">:</span>
                <span class="o">//...</span>
</pre></div>
</div>
<p>You can use this implementation by simply passing <code class="code docutils literal notranslate"><span class="pre">&quot;cpp.NHWC&quot;</span></code> to implement:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">loaded_model</span> <span class="o">=</span> <span class="n">fastinference</span><span class="o">.</span><span class="n">Loader</span><span class="o">.</span><span class="n">model_from_file</span><span class="p">(</span><span class="s2">&quot;/my/nice/model.onnx&quot;</span><span class="p">)</span>
<span class="n">loaded_model</span><span class="o">.</span><span class="n">implement</span><span class="p">(</span><span class="s2">&quot;/some/nice/place/&quot;</span><span class="p">,</span> <span class="s2">&quot;mymodel&quot;</span><span class="p">,</span> <span class="s2">&quot;cpp.NHWC&quot;</span><span class="p">)</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>model</strong> (<a class="reference internal" href="neuralnet.html#fastinference.models.nn.NeuralNet.NeuralNet" title="fastinference.models.nn.NeuralNet.NeuralNet"><em>NeuralNet</em></a>) – The NeuralNet model to be implemented</p></li>
<li><p><strong>out_path</strong> (<em>str</em>) – The folder in which the <code class="code docutils literal notranslate"><span class="pre">*.cpp</span></code> and <code class="code docutils literal notranslate"><span class="pre">*.h</span></code> files are stored.</p></li>
<li><p><strong>out_name</strong> (<em>str</em>) – The filenames.</p></li>
<li><p><strong>weight</strong> (<em>float</em><em>, </em><em>optional</em>) – The weight of this model inside an ensemble. The weight is ignored if it is 1.0, otherwise the prediction is scaled by the respective weight. Defaults to 1.0.</p></li>
<li><p><strong>align</strong> (<em>int</em><em>, </em><em>optional</em>) – If align &gt; 0 then allocated memory will be aligned using __attribute__((aligned({{align}}))) where {{align}} is replaced by the given align value. If align = 0 then no memory alignment is performed. Defaults to 0.</p></li>
<li><p><strong>namespace</strong> (<em>str</em><em>, </em><em>optional</em>) – The namespace under which this model will be generated. Defaults to “FAST_INFERENCE”.</p></li>
<li><p><strong>feature_type</strong> (<em>str</em><em>, </em><em>optional</em>) – The data types of the input features. Defaults to “double”.</p></li>
<li><p><strong>label_type</strong> (<em>str</em><em>, </em><em>optional</em>) – The data types of the label. Defaults to “double”.</p></li>
<li><p><strong>internal_type</strong> (<em>str</em><em>, </em><em>optional</em>) – The data type used for internal buffers and memory allocation. Defaults to “double”.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="fastinference.implementations.neuralnet.cpp.binary.implement.to_implementation">
<span class="sig-prename descclassname"><span class="pre">fastinference.implementations.neuralnet.cpp.binary.implement.</span></span><span class="sig-name descname"><span class="pre">to_implementation</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">model</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">out_path</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">out_name</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">weight</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">namespace</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'FAST_INFERENCE'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">align</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">feature_type</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'double'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">label_type</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'double'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">float_type</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'double'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">int_type</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'signed</span> <span class="pre">int'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">uint_type</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'unsigned</span> <span class="pre">int'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">infer_types</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">popcount</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#fastinference.implementations.neuralnet.cpp.binary.implement.to_implementation" title="Permalink to this definition"></a></dt>
<dd><p>Generates a C++ implementation of the given binarized NeuralNet model by using the XNOR and popcount operations whenever possible. When infer_types is true, this implementation tries to infer the smallest possible data type which still guarantees a correct execution. Otherwise the supplied data types are used.  Note that the first layer is never binarized because we do not assume the input data to be in a packed integer format, but to be a regular array.</p>
<p><strong>Important</strong>: This implementation performs basic optimization on the model. It</p>
<blockquote>
<div><ul class="simple">
<li><p>removes unnecessary layers (see <a class="reference internal" href="neuralnet.html#fastinference.optimizers.neuralnet.remove_nodes.optimize" title="fastinference.optimizers.neuralnet.remove_nodes.optimize"><code class="xref py py-meth docutils literal notranslate"><span class="pre">fastinference.optimizers.neuralnet.remove_nodes.optimize()</span></code></a>),</p></li>
<li><p>merges BatchNorm + Step layers (see <a class="reference internal" href="neuralnet.html#fastinference.optimizers.neuralnet.merge_nodes.optimize" title="fastinference.optimizers.neuralnet.merge_nodes.optimize"><code class="xref py py-meth docutils literal notranslate"><span class="pre">fastinference.optimizers.neuralnet.merge_nodes.optimize()</span></code></a>)</p></li>
<li><p>it maps all Conv and linear weights / biases to {-1, +1} if not done already</p></li>
</ul>
</div></blockquote>
<p><strong>Important</strong>: This implementation generates gcc compliant code by using the <code class="code docutils literal notranslate"><span class="pre">__builtin_popcount</span></code> or <code class="code docutils literal notranslate"><span class="pre">__builtin_popcountll</span></code> (depending on the uint_type) intrinsic. This intrinsic can vary from compiler to compiler. If you want to compile with another compiler (e.g. MSVC or clang) then you can supply the corresponding popcount operation via the popcount argument. However, please keep in mind that you might have to adapt the included headers manually after the code generation and that the popcount operation should fit the corresponding uint_type.</p>
<p>You can use this implementation by simply passing <code class="code docutils literal notranslate"><span class="pre">&quot;cpp.binary&quot;</span></code> to implement:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">loaded_model</span> <span class="o">=</span> <span class="n">fastinference</span><span class="o">.</span><span class="n">Loader</span><span class="o">.</span><span class="n">model_from_file</span><span class="p">(</span><span class="s2">&quot;/my/nice/model.onnx&quot;</span><span class="p">)</span>
<span class="n">loaded_model</span><span class="o">.</span><span class="n">implement</span><span class="p">(</span><span class="s2">&quot;/some/nice/place/&quot;</span><span class="p">,</span> <span class="s2">&quot;mymodel&quot;</span><span class="p">,</span> <span class="s2">&quot;cpp.binary&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p>References:</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>model</strong> (<a class="reference internal" href="neuralnet.html#fastinference.models.nn.NeuralNet.NeuralNet" title="fastinference.models.nn.NeuralNet.NeuralNet"><em>NeuralNet</em></a>) – The NeuralNet model to be implemented</p></li>
<li><p><strong>out_path</strong> (<em>str</em>) – The folder in which the <code class="code docutils literal notranslate"><span class="pre">*.cpp</span></code> and <code class="code docutils literal notranslate"><span class="pre">*.h</span></code> files are stored.</p></li>
<li><p><strong>out_name</strong> (<em>str</em>) – The filenames.</p></li>
<li><p><strong>weight</strong> (<em>float</em><em>, </em><em>optional</em>) – The weight of this model inside an ensemble. The weight is ignored if it is 1.0, otherwise the prediction is scaled by the respective weight. Defaults to 1.0.</p></li>
<li><p><strong>align</strong> (<em>int</em><em>, </em><em>optional</em>) – If align &gt; 0 then allocated memory will be aligned using __attribute__((aligned({{align}}))) where {{align}} is replaced by the given align value. If align = 0 then no memory alignment is performed. Defaults to 0.</p></li>
<li><p><strong>namespace</strong> (<em>str</em><em>, </em><em>optional</em>) – The namespace under which this model will be generated. Defaults to “FAST_INFERENCE”.</p></li>
<li><p><strong>feature_type</strong> (<em>str</em><em>, </em><em>optional</em>) – The data types of the input features. Defaults to “double”.</p></li>
<li><p><strong>label_type</strong> (<em>str</em><em>, </em><em>optional</em>) – The data types of the label. Defaults to “double”.</p></li>
<li><p><strong>float_type</strong> (<em>str</em><em>, </em><em>optional</em>) – The floating point type used when required. Defaults to “double”.</p></li>
<li><p><strong>int_type</strong> (<em>str</em><em>, </em><em>optional</em>) – The signed integer type used when required. Defaults to “signed int”.</p></li>
<li><p><strong>uint_type</strong> (<em>str</em><em>, </em><em>optional</em>) – The unsigned integer type used when required. Defaults to “unsigned int”.</p></li>
<li><p><strong>infer_types</strong> (<em>bool</em><em>, </em><em>optional</em>) – If True tries to infer the smallest possible data type which still guarantees a correct implementation. Defaults to True.</p></li>
<li><p><strong>popcount</strong> (<em>str</em><em>, </em><em>optional</em>) – The popcount operation which should be used to compute popcount. If this is None, then <code class="code docutils literal notranslate"><span class="pre">__builtin_popcount</span></code> or <code class="code docutils literal notranslate"><span class="pre">__builtin_popcountll</span></code> is used depending on the binary_word_size required by the uint_type. Defaults to None.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="fastinference.implementations.neuralnet.fpga.binary.implement.to_implementation">
<span class="sig-prename descclassname"><span class="pre">fastinference.implementations.neuralnet.fpga.binary.implement.</span></span><span class="sig-name descname"><span class="pre">to_implementation</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">model</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">out_path</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">out_name</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">weight</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">namespace</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'FAST_INFERENCE'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">feature_type</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'double'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">label_type</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'double'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">float_type</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'double'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">int_type</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'signed</span> <span class="pre">int'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">uint_type</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'unsigned</span> <span class="pre">int'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">infer_types</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">lut_size</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#fastinference.implementations.neuralnet.fpga.binary.implement.to_implementation" title="Permalink to this definition"></a></dt>
<dd><p>Generates a C++ implementation of the given binarized NeuralNet model by using the XNOR and popcount operations whenever possible. This implementation is targeted towards High Level Synthesis and FPGAs and more specifcially to Xilinx HLS. The popcount operation is implemented via lookup tables (LUT). When infer_types is true, this implementation tries to infer the smallest possible data type which still guarantees a correct execution. Otherwise the supplied data types are used.  Note that the first layer is never binarized because we do not assume the input data to be in a packed integer format, but to be a regular array.</p>
<p><strong>Important</strong>: This implementation performs basic optimization on the model. It</p>
<blockquote>
<div><ul class="simple">
<li><p>removes unnecessary layers (see <a class="reference internal" href="neuralnet.html#fastinference.optimizers.neuralnet.remove_nodes.optimize" title="fastinference.optimizers.neuralnet.remove_nodes.optimize"><code class="xref py py-meth docutils literal notranslate"><span class="pre">fastinference.optimizers.neuralnet.remove_nodes.optimize()</span></code></a>),</p></li>
<li><p>merges BatchNorm + Step layers (see <a class="reference internal" href="neuralnet.html#fastinference.optimizers.neuralnet.merge_nodes.optimize" title="fastinference.optimizers.neuralnet.merge_nodes.optimize"><code class="xref py py-meth docutils literal notranslate"><span class="pre">fastinference.optimizers.neuralnet.merge_nodes.optimize()</span></code></a>)</p></li>
<li><p>it maps all Conv and linear weights / biases to {-1, +1} if not done already</p></li>
</ul>
</div></blockquote>
<p><strong>Important</strong>: This implementation does <strong>not</strong> perform any optimizations with respect to the HLS tool. We highly recommend to perform a design space exploration manually after the code generation for a given neural net to have the best performance. Additionally, the input data must likely be adapted manually to the given FPGA.</p>
<p>You can use this implementation by simply passing <code class="code docutils literal notranslate"><span class="pre">&quot;fpga.binary&quot;</span></code> to implement:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">loaded_model</span> <span class="o">=</span> <span class="n">fastinference</span><span class="o">.</span><span class="n">Loader</span><span class="o">.</span><span class="n">model_from_file</span><span class="p">(</span><span class="s2">&quot;/my/nice/model.onnx&quot;</span><span class="p">)</span>
<span class="n">loaded_model</span><span class="o">.</span><span class="n">implement</span><span class="p">(</span><span class="s2">&quot;/some/nice/place/&quot;</span><span class="p">,</span> <span class="s2">&quot;mymodel&quot;</span><span class="p">,</span> <span class="s2">&quot;fpga.binary&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p>References:</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>model</strong> (<a class="reference internal" href="neuralnet.html#fastinference.models.nn.NeuralNet.NeuralNet" title="fastinference.models.nn.NeuralNet.NeuralNet"><em>NeuralNet</em></a>) – The NeuralNet model to be implemented</p></li>
<li><p><strong>out_path</strong> (<em>str</em>) – The folder in which the <code class="code docutils literal notranslate"><span class="pre">*.cpp</span></code> and <code class="code docutils literal notranslate"><span class="pre">*.h</span></code> files are stored.</p></li>
<li><p><strong>out_name</strong> (<em>str</em>) – The filenames.</p></li>
<li><p><strong>weight</strong> (<em>float</em><em>, </em><em>optional</em>) – The weight of this model inside an ensemble. The weight is ignored if it is 1.0, otherwise the prediction is scaled by the respective weight. Defaults to 1.0.</p></li>
<li><p><strong>namespace</strong> (<em>str</em><em>, </em><em>optional</em>) – The namespace under which this model will be generated. Defaults to “FAST_INFERENCE”.</p></li>
<li><p><strong>feature_type</strong> (<em>str</em><em>, </em><em>optional</em>) – The data types of the input features. Defaults to “double”.</p></li>
<li><p><strong>label_type</strong> (<em>str</em><em>, </em><em>optional</em>) – The data types of the label. Defaults to “double”.</p></li>
<li><p><strong>float_type</strong> (<em>str</em><em>, </em><em>optional</em>) – The floating point type used when required. Defaults to “double”.</p></li>
<li><p><strong>int_type</strong> (<em>str</em><em>, </em><em>optional</em>) – The signed integer type used when required. Defaults to “signed int”.</p></li>
<li><p><strong>uint_type</strong> (<em>str</em><em>, </em><em>optional</em>) – The unsigned integer type used when required. Defaults to “unsigned int”.</p></li>
<li><p><strong>infer_types</strong> (<em>bool</em><em>, </em><em>optional</em>) – If True tries to infer the smallest possible data type which still guarantees a correct implementation. Defaults to True.</p></li>
<li><p><strong>popcount</strong> (<em>str</em><em>, </em><em>optional</em>) – The popcount operation which should be used to compute popcount. If this is None, then <code class="code docutils literal notranslate"><span class="pre">__builtin_popcount</span></code> or <code class="code docutils literal notranslate"><span class="pre">__builtin_popcountll</span></code> is used depending on the binary_word_size required by the uint_type. Defaults to None.</p></li>
<li><p><strong>lut_size</strong> (<em>integer</em><em>, </em><em>optional</em>) – If lut_size &gt; 1 then a lookup table for each lut_size number of bits is generated to compute the popcount operation. For example, if binary_word_size is 32 and lut_size is 4, then a pack of 4 bits is evaluated at-once resulting in 32 / 4 = 8 lookups. If lut_size &lt;= 1, then a for-loop is used to compute the popcount. Defaults to 0.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="neuralnet.html" class="btn btn-neutral float-left" title="Neural Networks" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="extending.html" class="btn btn-neutral float-right" title="Extending fastinference" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2021, Sebastian Buschjäger.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>