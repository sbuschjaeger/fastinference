<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Extending fastinference &mdash; Fastinference  documentation</title>
      <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
      <link rel="stylesheet" href="_static/custom.css" type="text/css" />
  <!--[if lt IE 9]>
    <script src="_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
        <script src="_static/jquery.js"></script>
        <script src="_static/underscore.js"></script>
        <script src="_static/doctools.js"></script>
    <script src="_static/js/theme.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="prev" title="Implementations" href="implementations.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="index.html" class="icon icon-home"> Fastinference
            <img src="_static/logo-docs.png" class="logo" alt="Logo"/>
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <ul class="current">
<li class="toctree-l1"><a class="reference internal" href="index.html">Fastinference</a></li>
<li class="toctree-l1"><a class="reference internal" href="models.html">Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="implementations.html">Implementations</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Extending fastinference</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#adding-a-new-type-of-implementation">Adding a new type of implementation</a></li>
<li class="toctree-l2"><a class="reference internal" href="#adding-a-new-type-of-optimization">Adding a new type of optimization</a></li>
<li class="toctree-l2"><a class="reference internal" href="#adding-a-new-type-of-model">Adding a new type of model</a></li>
<li class="toctree-l2"><a class="reference internal" href="#testing-your-implementation-optimization">Testing your implementation / optimization</a></li>
</ul>
</li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">Fastinference</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content style-external-links">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="index.html" class="icon icon-home"></a> &raquo;</li>
      <li>Extending fastinference</li>
      <li class="wy-breadcrumbs-aside">
              <a href="https://github.com/sbuschjaeger/fastinference/blob/master/docs/extending.rst" class="fa fa-github"> Edit on GitHub</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="extending-fastinference">
<span id="extending-label"></span><h1>Extending fastinference<a class="headerlink" href="#extending-fastinference" title="Permalink to this headline"></a></h1>
<p>One of the main design goals of Fastinference is to allow to easily add new types of optimizations and implementations while benefiting from existing optimizations and implementations out of the box. The central object in Fastinference is the model.</p>
<section id="adding-a-new-type-of-implementation">
<h2>Adding a new type of implementation<a class="headerlink" href="#adding-a-new-type-of-implementation" title="Permalink to this headline"></a></h2>
<p>Adding a new implementation for a given object is easy. Simply provide a <code class="code docutils literal notranslate"><span class="pre">implement.py</span></code> file which contains a function <code class="code docutils literal notranslate"><span class="pre">to_implementation</span></code> which receives</p>
<ul class="simple">
<li><p>model: The model to be implemented. This is a deepcopy of the original model’s object so you can perform changes on this object if required.</p></li>
<li><p>out_path: The folder in which the source code for this model should be stored</p></li>
<li><p>out_name: The filename under which the models implementation should be stored</p></li>
<li><p>weight: The weight of this model in case it is part of an ensemble. The prediction should be scaled by this weight.</p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">to_implementation</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">out_path</span><span class="p">,</span> <span class="n">out_name</span><span class="p">,</span> <span class="n">weight</span> <span class="o">=</span> <span class="mf">1.0</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
    <span class="c1"># Generate the new implementation here</span>
</pre></div>
</div>
<p>Fastinference will search for existing implementations under <code class="code docutils literal notranslate"><span class="pre">implementations/my/new/implementation/imlement.py</span></code> which can then be loaded via <code class="code docutils literal notranslate"><span class="pre">--implementation</span> <span class="pre">my.new.implementation</span></code>. Per convention we currently store implementations under <code class="code docutils literal notranslate"><span class="pre">implementations/{model}/{language}/{implementation}</span></code>. You can pass any additional argument using <code class="code docutils literal notranslate"><span class="pre">kwargs</span></code> and fastinference will try to lazily pass any command-line arguments to you function. Don’t forget to document your implementation. Just adapt <code class="code docutils literal notranslate"><span class="pre">docs/implementations.rst</span></code> to include your new implementation and the docstring of your <code class="code docutils literal notranslate"><span class="pre">to_implementation</span></code> will be include in the docs.</p>
<p><strong>A note for ensembles</strong>: For the <code class="code docutils literal notranslate"><span class="pre">cpp</span></code> implementations we currently assume the following signature. Here, the predictions should be <strong>added</strong> into the <code class="code docutils literal notranslate"><span class="pre">pred</span></code> array and not copied, because the implementation of the ensemble will call each base-learners implementation on the <strong>same</strong> array.</p>
<div class="highlight-c++ notranslate"><div class="highlight"><pre><span></span><span class="kt">void</span> <span class="n">predict_</span><span class="p">{{</span><span class="n">model</span><span class="p">.</span><span class="n">name</span><span class="p">}}({{</span> <span class="n">feature_type</span> <span class="p">}}</span> <span class="k">const</span> <span class="o">*</span> <span class="k">const</span> <span class="n">x</span><span class="p">,</span> <span class="p">{{</span> <span class="n">label_type</span> <span class="p">}}</span> <span class="o">*</span> <span class="n">pred</span><span class="p">)</span> <span class="p">{</span>
    <span class="c1">// the actual code</span>
<span class="p">}</span>
</pre></div>
</div>
<p><strong>Important:</strong> Currently all implementations utilize the template engine jinja (<a class="reference external" href="https://jinja.palletsprojects.com/en/3.0.x/">https://jinja.palletsprojects.com/en/3.0.x/</a>), but there is no requirement to use jinja for new types of implementations. We originally intended to provide all implementations via jinja (e.g. also for other languages), but although jinja is very powerful it would sometimes be very difficult to provide certain types of implementations. Hence, we decided to simply use python code to generate the necessary implementations without any formal depenence on jinja. Nevertheless, we recommend to use jinja whenever possible. For any C-type language (e.g. C, Java etc.) we recommend to simply copy the entire implementation folder of each model and then to adapt the jinja templates wherever necessary.</p>
</section>
<section id="adding-a-new-type-of-optimization">
<h2>Adding a new type of optimization<a class="headerlink" href="#adding-a-new-type-of-optimization" title="Permalink to this headline"></a></h2>
<p>Adding a new optimization for a given object is easy. Simply provid a function <code class="code docutils literal notranslate"><span class="pre">optimize</span></code> which receives   the model to be optimized and returns the optimized model:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">optimize</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
    <span class="c1"># Perform some optimizations on model the new implementation here</span>

    <span class="k">return</span> <span class="n">model</span>
</pre></div>
</div>
<p>Fastinference will search for existing optimizations under <code class="code docutils literal notranslate"><span class="pre">optimizations/my/new/optimization.py</span></code> which can then be loaded via <code class="code docutils literal notranslate"><span class="pre">--optimize</span> <span class="pre">my.new.optimization</span></code>. Per convention we currently store optimizations under <code class="code docutils literal notranslate"><span class="pre">{optimizers}/{model}/</span></code>. You can pass any additional argument using <code class="code docutils literal notranslate"><span class="pre">kwargs</span></code> and fastinference will try to lazily pass any command-line arguments to you function. Don’t forget to document your implementation. Just adapt <code class="code docutils literal notranslate"><span class="pre">docs/{model}.rst</span></code> to include your new optimization and the docstring of your <code class="code docutils literal notranslate"><span class="pre">optimize</span></code> will be include in the docs.</p>
</section>
<section id="adding-a-new-type-of-model">
<h2>Adding a new type of model<a class="headerlink" href="#adding-a-new-type-of-model" title="Permalink to this headline"></a></h2>
<p>Adding a new model to fastinference is slightly more work. First, you need to implement <cite>fastinference.models.Model</cite>. To do so, you will have to implement the <code class="code docutils literal notranslate"><span class="pre">predict_proba</span></code> method which executes the given model on a batch of data and the <code class="code docutils literal notranslate"><span class="pre">to_dict</span></code> method which return a dictionary representation of the model. Last, you also might need to supply a new model category such as <code class="code docutils literal notranslate"><span class="pre">{linear,</span> <span class="pre">tree,</span> <span class="pre">ensemble,</span> <span class="pre">discriminant,</span> <span class="pre">neuralnet}</span></code>:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">MyModel</span><span class="p">(</span><span class="n">Model</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">num_classes</span><span class="p">,</span> <span class="n">accuracy</span> <span class="o">=</span> <span class="bp">None</span><span class="p">,</span> <span class="n">name</span> <span class="o">=</span> <span class="s2">&quot;Model&quot;</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">num_classes</span><span class="p">,</span> <span class="s2">&quot;A-new-category&quot;</span><span class="p">,</span> <span class="n">accuracy</span><span class="p">,</span> <span class="n">name</span><span class="p">)</span>
        <span class="k">pass</span>

    <span class="k">def</span> <span class="nf">predict_proba</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">X</span><span class="p">):</span>
        <span class="k">pass</span>

    <span class="k">def</span> <span class="nf">to_dict</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">model_dict</span> <span class="o">=</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">to_dict</span><span class="p">()</span>
        <span class="c1"># Add some stuff to model_dict</span>
        <span class="k">return</span> <span class="n">model_dict</span>
</pre></div>
</div>
<p>Once the model is implemented you need to provide methods for loading and storing. The main entry points for loading and storing in fastinference</p>
<ul class="simple">
<li><p><code class="code docutils literal notranslate"><span class="pre">Loader.model_from_file</span></code> for loading a new model from a file</p></li>
<li><p><code class="code docutils literal notranslate"><span class="pre">Loader.model_to_json</span></code> for storing a new model into a JSON file</p></li>
</ul>
<p>In order to load the model you will have to adapt <code class="code docutils literal notranslate"><span class="pre">Loader.model_from_file</span></code>. If your model does not really fit into a JSON format or comes with its own format (e.g. as for neural networks and the ONNX format) then you can ignore <code class="code docutils literal notranslate"><span class="pre">Loader.model_to_json</span></code>. However, we try to keep these loading / storing functions as consistent as possible so try to provide both if possible.</p>
</section>
<section id="testing-your-implementation-optimization">
<h2>Testing your implementation / optimization<a class="headerlink" href="#testing-your-implementation-optimization" title="Permalink to this headline"></a></h2>
<p>Training a model, generating the code and finally compiling it can be a cumbersome endeavor if you want to debug / test your implementation. We offer some scripts which help during development</p>
<ul class="simple">
<li><p><code class="code docutils literal notranslate"><span class="pre">environment.yml</span></code>: A anaconda environment file which we use during development.</p></li>
<li><p><code class="code docutils literal notranslate"><span class="pre">tests/generate_data.py</span></code>: A script to generate some random test and training data.</p></li>
<li><p><code class="code docutils literal notranslate"><span class="pre">tests/train_{linear,discriminant,tree,mlp,cnn}.py</span></code>: A script to train the respective classifier or an ensemble of those.</p></li>
<li><p><code class="code docutils literal notranslate"><span class="pre">tests/convert_data.py</span></code>: A script to convert the test data into a static header file for the c++ implementations.</p></li>
<li><p><code class="code docutils literal notranslate"><span class="pre">tests/main.cpp</span></code>: The main.cpp file when testing c++ implementations.</p></li>
<li><p><code class="code docutils literal notranslate"><span class="pre">tests/CMakeLists.txt</span></code>: The CMakeLists when testing c++ implementations.</p></li>
</ul>
<p>A complete example of the entire workflow can be found in <a class="reference external" href="https://github.com/sbuschjaeger/fastinference/blob/main/tests/run_test.sh">run_tests.sh</a> and we try to maintain a CI/CD pipeline under <a class="reference external" href="https://github.com/sbuschjaeger/fastinference/blob/main/.github/workflows/tests.yml">tests.yml</a>. Please check this file for the latest test configurations.</p>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="implementations.html" class="btn btn-neutral float-left" title="Implementations" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2021, Sebastian Buschjäger.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>