<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Neural Networks &mdash; Fastinference  documentation</title>
      <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
      <link rel="stylesheet" href="_static/custom.css" type="text/css" />
  <!--[if lt IE 9]>
    <script src="_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
        <script src="_static/jquery.js"></script>
        <script src="_static/underscore.js"></script>
        <script src="_static/_sphinx_javascript_frameworks_compat.js"></script>
        <script src="_static/doctools.js"></script>
    <script src="_static/js/theme.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Implementations" href="implementations.html" />
    <link rel="prev" title="Ensemble" href="ensemble.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="index.html" class="icon icon-home"> Fastinference
            <img src="_static/logo-docs.png" class="logo" alt="Logo"/>
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <ul class="current">
<li class="toctree-l1"><a class="reference internal" href="index.html">Fastinference</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="models.html">Models</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="linear.html">Linear</a></li>
<li class="toctree-l2"><a class="reference internal" href="discriminant.html">DiscriminantAnalysis</a></li>
<li class="toctree-l2"><a class="reference internal" href="tree.html">Tree</a></li>
<li class="toctree-l2"><a class="reference internal" href="ensemble.html">Ensemble</a></li>
<li class="toctree-l2 current"><a class="current reference internal" href="#">Neural Networks</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#some-notes-on-binarized-neural-networks">Some notes on Binarized Neural Networks</a></li>
<li class="toctree-l3"><a class="reference internal" href="#available-optimizations">Available optimizations</a></li>
<li class="toctree-l3"><a class="reference internal" href="#module-fastinference.models.nn.NeuralNet">The NeuralNet object</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="implementations.html">Implementations</a></li>
<li class="toctree-l1"><a class="reference internal" href="extending.html">Extending fastinference</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">Fastinference</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content style-external-links">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="index.html" class="icon icon-home"></a> &raquo;</li>
          <li><a href="models.html">Models</a> &raquo;</li>
      <li>Neural Networks</li>
      <li class="wy-breadcrumbs-aside">
              <a href="https://github.com/sbuschjaeger/fastinference/blob/master/docs/neuralnet.rst" class="fa fa-github"> Edit on GitHub</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="neural-networks">
<h1>Neural Networks<a class="headerlink" href="#neural-networks" title="Permalink to this heading"></a></h1>
<p>Fastinference offers a limited support for Deep Learning and Neural Network architectures. The current focus is on feed-forward MLPs and ConvNets in the context of small, embedded systems and FPGAs, but we are always open to enhance our support for new Deep Learning architectures.</p>
<p><strong>Important:</strong> <a class="reference external" href="https://onnx.ai/">ONNX</a> is the open standard for machine learning interoperability and supported by all major Deep Learning frameworks. However, the ONNX format is still under development and a given deep architecture can often be represented with various computational graphs. Hence, this standard is sometimes ambiguous. This implementation has been tested with <a class="reference external" href="https://pytorch.org/">PyTorch</a> and visualized with <a class="reference external" href="https://netron.app/">Netron</a>. For exporting a Neural Net we usually use</p>
<blockquote>
<div><div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">dummy_x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">x_train</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">requires_grad</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
<span class="n">torch</span><span class="o">.</span><span class="n">onnx</span><span class="o">.</span><span class="n">export</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">dummy_x</span><span class="p">,</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">out_path</span><span class="p">,</span><span class="n">name</span><span class="p">),</span> <span class="n">training</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">onnx</span><span class="o">.</span><span class="n">TrainingMode</span><span class="o">.</span><span class="n">PRESERVE</span><span class="p">,</span> <span class="n">export_params</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span><span class="n">opset_version</span><span class="o">=</span><span class="mi">11</span><span class="p">,</span> <span class="n">do_constant_folding</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">input_names</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;input&#39;</span><span class="p">],</span>  <span class="n">output_names</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;output&#39;</span><span class="p">],</span> <span class="n">dynamic_axes</span><span class="o">=</span><span class="p">{</span><span class="s1">&#39;input&#39;</span> <span class="p">:</span> <span class="p">{</span><span class="mi">0</span> <span class="p">:</span> <span class="s1">&#39;batch_size&#39;</span><span class="p">},</span><span class="s1">&#39;output&#39;</span> <span class="p">:</span> <span class="p">{</span><span class="mi">0</span> <span class="p">:</span> <span class="s1">&#39;batch_size&#39;</span><span class="p">}})</span>
</pre></div>
</div>
</div></blockquote>
<section id="some-notes-on-binarized-neural-networks">
<h2>Some notes on Binarized Neural Networks<a class="headerlink" href="#some-notes-on-binarized-neural-networks" title="Permalink to this heading"></a></h2>
<p>Binarized Neural Networks (BNNs) are Neural Networks with weights constraint to {-1,+1} so that the forward pass of the entire network can be executed via boolean operations (usually XNOR + popcount). A typical structure of these networks are as follows:</p>
<blockquote>
<div><p>Input -&gt; Linear / Conv -&gt; BatchNorm -&gt; Step -&gt; … -&gt; Linear / Conv -&gt; BatchNorm -&gt; Step -&gt; Output</p>
</div></blockquote>
<p>where the Linear / Conv layers only have “binary” weights and biases {-1,+1} and the step function is Heaviside function. BNNs are usually not supported by the major frameworks out of the box, but require some additional libraries as well as some tweaks in the ONNX format. For example, <a class="reference external" href="https://larq.dev/">larq</a> offers binarization for keras / tensorflow and <a class="reference external" href="https://github.com/Xilinx/brevitas">Brevitas</a> enables binarization for PyTorch. Alternatively, we can directly implement binarization as shown in the example below. Unfortunately, ONNX does not support the custom operators from these libraries so that we have to sanitize these before exporting. In fastinference we simply replace each binary layer, e.g. <code class="code docutils literal notranslate"><span class="pre">BinaryLinear</span></code>, with its regular counterpart <code class="code docutils literal notranslate"><span class="pre">torch.nn.Linear</span></code>. Moreover, PyTorch cannot export the Heaviside function yet into an ONNX file. Hence we mimic this function with a series of “Constant -&gt; Greater -&gt; Constant -&gt; Constant -&gt; Where” layers which is then parsed and merged back into a Step layer by fastinference. For a complete example check out <a class="reference external" href="https://github.com/sbuschjaeger/fastinference/blob/main/tests/train_mlp.py">train_mlp.py</a> or <a class="reference external" href="https://github.com/sbuschjaeger/fastinference/blob/main/tests/train_cnn.py">train_cnn.py</a>.</p>
<blockquote>
<div><div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">BinarizeF</span><span class="p">(</span><span class="n">Function</span><span class="p">):</span>
    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="nb">input</span><span class="p">):</span>
        <span class="n">output</span> <span class="o">=</span> <span class="nb">input</span><span class="o">.</span><span class="n">new</span><span class="p">(</span><span class="nb">input</span><span class="o">.</span><span class="n">size</span><span class="p">())</span>
        <span class="n">output</span><span class="p">[</span><span class="nb">input</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>
        <span class="n">output</span><span class="p">[</span><span class="nb">input</span> <span class="o">&lt;=</span> <span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span>
        <span class="k">return</span> <span class="n">output</span>

    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">backward</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">grad_output</span><span class="p">):</span>
        <span class="c1">#return grad_output, None</span>
        <span class="n">grad_input</span> <span class="o">=</span> <span class="n">grad_output</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span>
        <span class="k">return</span> <span class="n">grad_input</span><span class="c1">#, None</span>

<span class="n">binarize</span> <span class="o">=</span> <span class="n">BinarizeF</span><span class="o">.</span><span class="n">apply</span>

<span class="k">class</span> <span class="nc">BinaryLinear</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">BinaryLinear</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">):</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">bias</span> <span class="ow">is</span> <span class="bp">None</span><span class="p">:</span>
            <span class="n">binary_weight</span> <span class="o">=</span> <span class="n">binarize</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">weight</span><span class="p">)</span>

            <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">linear</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">binary_weight</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">binary_weight</span> <span class="o">=</span> <span class="n">binarize</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">weight</span><span class="p">)</span>
            <span class="n">binary_bias</span> <span class="o">=</span> <span class="n">binarize</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">bias</span><span class="p">)</span>
            <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">linear</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">binary_weight</span><span class="p">,</span> <span class="n">binary_bias</span><span class="p">)</span>

<span class="k">class</span> <span class="nc">BinaryTanh</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">BinaryTanh</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">hardtanh</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Hardtanh</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">):</span>
        <span class="n">output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">hardtanh</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
        <span class="n">output</span> <span class="o">=</span> <span class="n">binarize</span><span class="p">(</span><span class="n">output</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">output</span>


<span class="k">class</span> <span class="nc">SimpleMLP</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_dim</span><span class="p">,</span> <span class="n">n_classes</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">layer_1</span> <span class="o">=</span> <span class="n">BinaryLinear</span><span class="p">(</span><span class="n">input_dim</span><span class="p">,</span> <span class="mi">128</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">bn_1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm1d</span><span class="p">(</span><span class="mi">128</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">activation_1</span> <span class="o">=</span> <span class="n">BinaryTanh</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">layer_2</span> <span class="o">=</span> <span class="n">BinaryLinear</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="mi">256</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">bn_2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm1d</span><span class="p">(</span><span class="mi">256</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">activation_2</span> <span class="o">=</span> <span class="n">BinaryTanh</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">layer_3</span> <span class="o">=</span> <span class="n">BinaryLinear</span><span class="p">(</span><span class="mi">256</span><span class="p">,</span> <span class="n">n_classes</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">layer_1</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">bn_1</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">activation_1</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">layer_2</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">bn_2</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">activation_2</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">layer_3</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">log_softmax</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">x</span>

<span class="k">def</span> <span class="nf">sanatize_onnx</span><span class="p">(</span><span class="n">model</span><span class="p">):</span>

    <span class="c1"># Usually I would use https://pytorch.org/docs/stable/generated/torch.heaviside.html for exporting here, but this is not yet supported in ONNX files.</span>
    <span class="k">class</span> <span class="nc">Sign</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
        <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">):</span>
            <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="nb">input</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">1.0</span><span class="p">]),</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="o">-</span><span class="mf">1.0</span><span class="p">]))</span>

    <span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">m</span> <span class="ow">in</span> <span class="nb">reversed</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">_modules</span><span class="o">.</span><span class="n">items</span><span class="p">()):</span>
        <span class="k">print</span><span class="p">(</span><span class="s2">&quot;Checking {}&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">name</span><span class="p">))</span>

        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="n">BinaryLinear</span><span class="p">):</span>
            <span class="k">print</span><span class="p">(</span><span class="s2">&quot;Replacing {}&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">name</span><span class="p">))</span>
            <span class="c1"># layer_old = m</span>
            <span class="n">layer_new</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">m</span><span class="o">.</span><span class="n">in_features</span><span class="p">,</span> <span class="n">m</span><span class="o">.</span><span class="n">out_features</span><span class="p">,</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="s1">&#39;bias&#39;</span><span class="p">))</span>
            <span class="k">if</span> <span class="p">(</span><span class="nb">hasattr</span><span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="s1">&#39;bias&#39;</span><span class="p">)):</span>
                <span class="n">layer_new</span><span class="o">.</span><span class="n">bias</span><span class="o">.</span><span class="n">data</span> <span class="o">=</span> <span class="n">binarize</span><span class="p">(</span><span class="n">m</span><span class="o">.</span><span class="n">bias</span><span class="o">.</span><span class="n">data</span><span class="p">)</span>
            <span class="n">layer_new</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">data</span> <span class="o">=</span> <span class="n">binarize</span><span class="p">(</span><span class="n">m</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">data</span><span class="p">)</span>
            <span class="n">model</span><span class="o">.</span><span class="n">_modules</span><span class="p">[</span><span class="n">name</span><span class="p">]</span> <span class="o">=</span> <span class="n">layer_new</span>

        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="n">BinaryTanh</span><span class="p">):</span>
            <span class="n">model</span><span class="o">.</span><span class="n">_modules</span><span class="p">[</span><span class="n">name</span><span class="p">]</span> <span class="o">=</span> <span class="n">Sign</span><span class="p">()</span>

    <span class="k">return</span> <span class="n">model</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">SimpleMLP</span><span class="p">(</span><span class="n">input_dim</span><span class="p">,</span> <span class="n">n_classes</span><span class="p">)</span>
<span class="c1"># Train the model</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">sanatize_onnx</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>
<span class="n">torch</span><span class="o">.</span><span class="n">onnx</span><span class="o">.</span><span class="n">export</span><span class="p">(</span><span class="n">model</span><span class="p">,</span><span class="n">dummy_x</span><span class="p">,</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">out_path</span><span class="p">,</span><span class="n">name</span><span class="p">),</span> <span class="n">export_params</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span><span class="n">opset_version</span><span class="o">=</span><span class="mi">11</span><span class="p">,</span> <span class="n">do_constant_folding</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">input_names</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;input&#39;</span><span class="p">],</span>  <span class="n">output_names</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;output&#39;</span><span class="p">],</span> <span class="n">dynamic_axes</span><span class="o">=</span><span class="p">{</span><span class="s1">&#39;input&#39;</span> <span class="p">:</span> <span class="p">{</span><span class="mi">0</span> <span class="p">:</span> <span class="s1">&#39;batch_size&#39;</span><span class="p">},</span><span class="s1">&#39;output&#39;</span> <span class="p">:</span> <span class="p">{</span><span class="mi">0</span> <span class="p">:</span> <span class="s1">&#39;batch_size&#39;</span><span class="p">}})</span>
</pre></div>
</div>
</div></blockquote>
</section>
<section id="available-optimizations">
<h2>Available optimizations<a class="headerlink" href="#available-optimizations" title="Permalink to this heading"></a></h2>
<dl class="py function">
<dt class="sig sig-object py" id="fastinference.optimizers.neuralnet.merge_nodes.optimize">
<span class="sig-prename descclassname"><span class="pre">fastinference.optimizers.neuralnet.merge_nodes.</span></span><span class="sig-name descname"><span class="pre">optimize</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">model</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#fastinference.optimizers.neuralnet.merge_nodes.optimize" title="Permalink to this definition"></a></dt>
<dd><p>Merges subsequent BatchNorm and Step layers into a new Step layer with adapted thresholds in a single pass. Currently there is no recursive merging applied.</p>
<p>TODO: Perform merging recursively.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>model</strong> (<a class="reference internal" href="#fastinference.models.nn.NeuralNet.NeuralNet" title="fastinference.models.nn.NeuralNet.NeuralNet"><em>NeuralNet</em></a>) – The NeuralNet model.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>The NeuralNet model with merged layers.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><a class="reference internal" href="#fastinference.models.nn.NeuralNet.NeuralNet" title="fastinference.models.nn.NeuralNet.NeuralNet">NeuralNet</a></p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="fastinference.optimizers.neuralnet.remove_nodes.optimize">
<span class="sig-prename descclassname"><span class="pre">fastinference.optimizers.neuralnet.remove_nodes.</span></span><span class="sig-name descname"><span class="pre">optimize</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">model</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#fastinference.optimizers.neuralnet.remove_nodes.optimize" title="Permalink to this definition"></a></dt>
<dd><p>Removes LogSoftmax and positive scaling (Mul) layers from the network because they do not change the prediction.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>model</strong> (<a class="reference internal" href="#fastinference.models.nn.NeuralNet.NeuralNet" title="fastinference.models.nn.NeuralNet.NeuralNet"><em>NeuralNet</em></a>) – The NeuralNet model.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>The NeuralNet model with removed layers.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><a class="reference internal" href="#fastinference.models.nn.NeuralNet.NeuralNet" title="fastinference.models.nn.NeuralNet.NeuralNet">NeuralNet</a></p>
</dd>
</dl>
</dd></dl>

</section>
<section id="module-fastinference.models.nn.NeuralNet">
<span id="the-neuralnet-object"></span><h2>The NeuralNet object<a class="headerlink" href="#module-fastinference.models.nn.NeuralNet" title="Permalink to this heading"></a></h2>
<dl class="py class">
<dt class="sig sig-object py" id="fastinference.models.nn.NeuralNet.NeuralNet">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">fastinference.models.nn.NeuralNet.</span></span><span class="sig-name descname"><span class="pre">NeuralNet</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">path_to_onnx</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">accuracy</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">name</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'model'</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#fastinference.models.nn.NeuralNet.NeuralNet" title="Permalink to this definition"></a></dt>
<dd><p>A (simplified) neural network model. This class currently supports feed-forward multi-layer perceptrons as well as feed-forward convnets. In detail the following operations are supported</p>
<blockquote>
<div><ul class="simple">
<li><p>Linear Layer</p></li>
<li><p>Convolutional Layer</p></li>
<li><p>Sigmoid Activation</p></li>
<li><p>ReLU Activation</p></li>
<li><p>LeakyRelu Activation</p></li>
<li><p>MaxPool</p></li>
<li><p>AveragePool</p></li>
<li><p>LogSoftmax</p></li>
<li><p>LogSoftmax</p></li>
<li><p>Multiplication with a constant (Mul)</p></li>
<li><p>Reshape</p></li>
<li><p>BatchNormalization</p></li>
</ul>
</div></blockquote>
<p>All layers are stored in <code class="code docutils literal notranslate"><span class="pre">self.layer</span></code> which is already order for execution. Additionally, the original onnx_model is stored in <code class="code docutils literal notranslate"><span class="pre">self.onnx_model</span></code>.</p>
<p>This class loads ONNX files to build the internal computation graph. This can sometimes become a little tricky since the ONNX exporter work differently for each framework / version. In PyToch we usually use</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">dummy_x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">x_train</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">requires_grad</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
<span class="n">torch</span><span class="o">.</span><span class="n">onnx</span><span class="o">.</span><span class="n">export</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">dummy_x</span><span class="p">,</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">out_path</span><span class="p">,</span><span class="n">name</span><span class="p">),</span> <span class="n">training</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">onnx</span><span class="o">.</span><span class="n">TrainingMode</span><span class="o">.</span><span class="n">PRESERVE</span><span class="p">,</span> <span class="n">export_params</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span><span class="n">opset_version</span><span class="o">=</span><span class="mi">11</span><span class="p">,</span> <span class="n">do_constant_folding</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">input_names</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;input&#39;</span><span class="p">],</span>  <span class="n">output_names</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;output&#39;</span><span class="p">],</span> <span class="n">dynamic_axes</span><span class="o">=</span><span class="p">{</span><span class="s1">&#39;input&#39;</span> <span class="p">:</span> <span class="p">{</span><span class="mi">0</span> <span class="p">:</span> <span class="s1">&#39;batch_size&#39;</span><span class="p">},</span><span class="s1">&#39;output&#39;</span> <span class="p">:</span> <span class="p">{</span><span class="mi">0</span> <span class="p">:</span> <span class="s1">&#39;batch_size&#39;</span><span class="p">}})</span>
</pre></div>
</div>
<p><strong>Important</strong>: This class automatically merges “Constant -&gt; Greater -&gt; Constant -&gt; Constant -&gt; Where” operations into a single step layer. This is specifically designed to parse Binarized Neural Networks, but might be wrong for some types of networks.</p>
<dl class="py method">
<dt class="sig sig-object py" id="fastinference.models.nn.NeuralNet.NeuralNet.__init__">
<span class="sig-name descname"><span class="pre">__init__</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">path_to_onnx</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">accuracy</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">name</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'model'</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#fastinference.models.nn.NeuralNet.NeuralNet.__init__" title="Permalink to this definition"></a></dt>
<dd><p>Constructor of NeuralNet.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>onnx_neural_net</strong> (<em>str</em>) – Path to the onnx file.</p></li>
<li><p><strong>accuracy</strong> (<em>float</em><em>, </em><em>optional</em>) – The accuracy of this tree on some test data. Can be used to verify the correctness of the implementation. Defaults to None.</p></li>
<li><p><strong>name</strong> (<em>str</em><em>, </em><em>optional</em>) – The name of this model. Defaults to “Model”.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="fastinference.models.nn.NeuralNet.NeuralNet.predict_proba">
<span class="sig-name descname"><span class="pre">predict_proba</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#fastinference.models.nn.NeuralNet.NeuralNet.predict_proba" title="Permalink to this definition"></a></dt>
<dd><p>Applies this NeuralNet to the given data and provides the predicted probabilities for each example in X. This function internally calls <code class="code docutils literal notranslate"><span class="pre">onnxruntime.InferenceSession</span></code> for inference..</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>X</strong> (<em>numpy.array</em>) – A (N,d) matrix where N is the number of data points and d is the feature dimension. If X has only one dimension then a single example is assumed and X is reshaped via <code class="code docutils literal notranslate"><span class="pre">X</span> <span class="pre">=</span> <span class="pre">X.reshape(1,X.shape[0])</span></code></p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A (N, c) prediction matrix where N is the number of data points and c is the number of classes</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>numpy.array</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="fastinference.models.nn.NeuralNet.layer_from_node">
<span class="sig-prename descclassname"><span class="pre">fastinference.models.nn.NeuralNet.</span></span><span class="sig-name descname"><span class="pre">layer_from_node</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">graph</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">node</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">input_shape</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#fastinference.models.nn.NeuralNet.layer_from_node" title="Permalink to this definition"></a></dt>
<dd><p>Constructs the appropriate layer from the given graph and node.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>graph</strong> – The onnx graph.</p></li>
<li><p><strong>node</strong> – The current node.</p></li>
<li><p><strong>input_shape</strong> (<em>tuple</em>) – The input shape of the current node</p></li>
</ul>
</dd>
<dt class="field-even">Raises</dt>
<dd class="field-even"><p><strong>NotImplementedError</strong> – Throws an error if there is no implementation for the current node available.</p>
</dd>
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>The newly constructed layer.</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p>Layer</p>
</dd>
</dl>
</dd></dl>

</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="ensemble.html" class="btn btn-neutral float-left" title="Ensemble" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="implementations.html" class="btn btn-neutral float-right" title="Implementations" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2021, Sebastian Buschjäger.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>